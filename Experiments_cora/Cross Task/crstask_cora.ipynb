{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075910fa-6891-4e54-86f9-5a370e0b17cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Using cached aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2023.9.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (1.24.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from torch_geometric) (4.66.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Using cached multidict-6.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->torch_geometric) (2023.7.22)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Using cached multidict-6.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: propcache, multidict, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 frozenlist-1.7.0 multidict-6.5.0 propcache-0.3.2 torch_geometric-2.6.1 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7639d6-a0fa-4275-84a7-d5686aef15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Pipeline for GCN ====================\n",
      "\n",
      "--- STAGE 1: Training GCN on Link Prediction ---\n",
      "\n",
      "--- STAGE 2: Evaluating GCN's Embeddings on Node Classification ---\n",
      "\n",
      "==================== Starting Pipeline for JKNetMax ====================\n",
      "\n",
      "--- STAGE 1: Training JKNetMax on Link Prediction ---\n",
      "\n",
      "--- STAGE 2: Evaluating JKNetMax's Embeddings on Node Classification ---\n",
      "\n",
      "==================== Starting Pipeline for MixHop ====================\n",
      "\n",
      "--- STAGE 1: Training MixHop on Link Prediction ---\n",
      "\n",
      "--- STAGE 2: Evaluating MixHop's Embeddings on Node Classification ---\n",
      "\n",
      "==================== Starting Pipeline for AltGCNMixHopJKN ====================\n",
      "\n",
      "--- STAGE 1: Training AltGCNMixHopJKN on Link Prediction ---\n",
      "\n",
      "--- STAGE 2: Evaluating AltGCNMixHopJKN's Embeddings on Node Classification ---\n",
      "\n",
      "\n",
      "--- FINAL CROSS-TASK EVALUATION SUMMARY ---\n",
      "\n",
      "Model: GCN\n",
      "\tEvaluated on Node Classification, Test Accuracy: 0.2380\n",
      "\n",
      "Model: JKNetMax\n",
      "\tEvaluated on Node Classification, Test Accuracy: 0.3020\n",
      "\n",
      "Model: MixHop\n",
      "\tEvaluated on Node Classification, Test Accuracy: 0.2860\n",
      "\n",
      "Model: AltGCNMixHopJKN\n",
      "\tEvaluated on Node Classification, Test Accuracy: 0.2340\n"
     ]
    }
   ],
   "source": [
    "\"\"\"CROSS-TASK EVALUATION: TRAIN ON LINK PREDICTION -> TEST ON NODE CLASSIFICATION\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import JumpingKnowledge, GCNConv, MixHopConv\n",
    "import torch_geometric.transforms as T\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import os\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim, num_layers=2, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, embedding_dim))\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class JKNetMax(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim, num_layers=6, dropout=0.5):\n",
    "        super(JKNetMax, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, 16)) \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(16, 16))\n",
    "        self.jump = JumpingKnowledge(mode='max', channels=16, num_layers=num_layers)\n",
    "        self.final_lin = torch.nn.Linear(16, embedding_dim)\n",
    "    def forward(self, x, edge_index):\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "        x_jump = self.jump(xs)\n",
    "        return self.final_lin(x_jump)\n",
    "\n",
    "class MixHopEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim, dropout=0.5, powers=[0, 1, 2]):\n",
    "        super(MixHopEncoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv1 = MixHopConv(in_channels, hidden_channels, powers=powers)\n",
    "        self.final_lin = torch.nn.Linear(hidden_channels * len(powers), embedding_dim)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.final_lin(x)\n",
    "        return x\n",
    "\n",
    "class AlternatingGCNMixHopJKNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim, num_layers=4, hop_hidden_channels=16, mode='max', dropout=0.5, mixhop_powers=[0,1,2]):\n",
    "        super(AlternatingGCNMixHopJKNet, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i % 2 == 0:\n",
    "                in_c = in_channels if i == 0 else hidden_channels\n",
    "                self.convs.append(GCNConv(in_c, hidden_channels))\n",
    "                self.lins.append(torch.nn.Identity())\n",
    "            else:\n",
    "                self.convs.append(MixHopConv(hidden_channels, hop_hidden_channels, powers=mixhop_powers))\n",
    "                self.lins.append(torch.nn.Linear(hop_hidden_channels * len(mixhop_powers), hidden_channels))\n",
    "        self.jump = JumpingKnowledge(mode=mode, channels=hidden_channels, num_layers=num_layers)\n",
    "        if mode == 'cat':\n",
    "            self.final_layer = torch.nn.Linear(num_layers * hidden_channels, embedding_dim)\n",
    "        else:\n",
    "            self.final_layer = torch.nn.Linear(hidden_channels, embedding_dim)\n",
    "    def forward(self, x, edge_index):\n",
    "        xs = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i % 2 == 0:\n",
    "                x = self.convs[i](x, edge_index)\n",
    "                x = F.relu(x)\n",
    "            else:\n",
    "                x = self.convs[i](x, edge_index)\n",
    "                x = self.lins[i](x)\n",
    "                x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            xs.append(x)\n",
    "        x_jump = self.jump(xs)\n",
    "        x_final = self.final_layer(x_jump)\n",
    "        return x_final\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, encoder, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * embedding_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_label_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        src_emb = z[edge_label_index[0]]\n",
    "        dst_emb = z[edge_label_index[1]]\n",
    "        return self.decoder(torch.cat([src_emb, dst_emb], dim=-1)).squeeze()\n",
    "\n",
    "def run_link_prediction_training(encoder_class_lambda, model_name, seed, embedding_dim):\n",
    "    print(f\"\\n--- STAGE 1: Training {model_name} on Link Prediction ---\")\n",
    "    set_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = Planetoid(root='./Cora', name='Cora', transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, add_negative_train_samples=True)\n",
    "    train_data, val_data, test_data = transform(data.clone())\n",
    "    \n",
    "    encoder = encoder_class_lambda(dataset.num_features, embedding_dim).to(device)\n",
    "    model = LinkPredictor(encoder, embedding_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "    best_val_auc, final_test_auc, patience = 0, 0, 10000\n",
    "    wait = 0\n",
    "    \n",
    "    for epoch in range(1, 10000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data.x.to(device), train_data.edge_index.to(device), train_data.edge_label_index.to(device))\n",
    "        loss = F.binary_cross_entropy_with_logits(out, train_data.edge_label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(train_data.x.to(device), train_data.edge_index.to(device), val_data.edge_label_index.to(device)).sigmoid()\n",
    "            val_auc = roc_auc_score(val_data.edge_label.cpu(), out.cpu())\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            with torch.no_grad():\n",
    "                out = model(train_data.x.to(device), train_data.edge_index.to(device), test_data.edge_label_index.to(device)).sigmoid()\n",
    "                final_test_auc = roc_auc_score(test_data.edge_label.cpu(), out.cpu())\n",
    "            torch.save(model.encoder.state_dict(), f'best_encoder_{model_name}.pt')\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}.\")\n",
    "                break\n",
    "        \n",
    "    best_encoder = encoder_class_lambda(dataset.num_features, embedding_dim).to(device)\n",
    "    if os.path.exists(f'best_encoder_{model_name}.pt'):\n",
    "        best_encoder.load_state_dict(torch.load(f'best_encoder_{model_name}.pt'))\n",
    "    return best_encoder, final_test_auc\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_on_node_classification(encoder, model_name, embedding_dim, seed):\n",
    "    print(f\"\\n--- STAGE 2: Evaluating {model_name}'s Embeddings on Node Classification ---\")\n",
    "    set_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = Planetoid(root='./Cora', name='Cora', transform=T.NormalizeFeatures())\n",
    "    data = dataset[0].to(device)\n",
    "    \n",
    "    encoder.eval()\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = encoder(data.x, data.edge_index)\n",
    "        \n",
    "    classifier = torch.nn.Linear(embedding_dim, dataset.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    final_test_acc = 0\n",
    "    patience, wait = 200, 0\n",
    "\n",
    "    for epoch in range(1, 5000):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = classifier(z)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = classifier(z).argmax(dim=-1)\n",
    "            val_acc = accuracy_score(data.y[data.val_mask].cpu(), pred[data.val_mask].cpu())\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            final_test_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu())\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "                \n",
    "    return final_test_acc\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed = 123\n",
    "    \n",
    "    model_configs = [\n",
    "        {\n",
    "            'name': 'GCN',\n",
    "            'encoder_lambda': lambda in_c, emb_dim: GCN(in_c, hidden_channels=32, embedding_dim=emb_dim, num_layers=2, dropout=0.5),\n",
    "            'embedding_dim': 32\n",
    "        },\n",
    "        {\n",
    "            'name': 'JKNetMax',\n",
    "            'encoder_lambda': lambda in_c, emb_dim: JKNetMax(in_c, hidden_channels=32, embedding_dim=emb_dim, num_layers=6, dropout=0.5),\n",
    "            'embedding_dim': 32 \n",
    "        },\n",
    "        {\n",
    "            'name': 'MixHop',\n",
    "            'encoder_lambda': lambda in_c, emb_dim: MixHopEncoder(in_c, hidden_channels=32, embedding_dim=emb_dim, dropout=0.5),\n",
    "            'embedding_dim': 96 \n",
    "        },\n",
    "        {\n",
    "            'name': 'AltGCNMixHopJKN',\n",
    "            'encoder_lambda': lambda in_c, emb_dim: AlternatingGCNMixHopJKNet(in_c, hidden_channels=32, embedding_dim=emb_dim, num_layers=3, hop_hidden_channels=32, mode='max', dropout=0.4),\n",
    "            'embedding_dim': 32\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    for config in model_configs:\n",
    "        print(f\"\\n{'='*20} Starting Pipeline for {config['name']} {'='*20}\")\n",
    "        try:\n",
    "            trained_encoder, test_auc = run_link_prediction_training(\n",
    "                config['encoder_lambda'], \n",
    "                config['name'], \n",
    "                seed, \n",
    "                config['embedding_dim']\n",
    "            )\n",
    "            \n",
    "            test_acc = evaluate_on_node_classification(\n",
    "                trained_encoder, \n",
    "                config['name'], \n",
    "                config['embedding_dim'], \n",
    "                seed\n",
    "            )\n",
    "            \n",
    "            results[config['name']] = {'Link Prediction AUC': test_auc, 'Node Classification Accuracy': test_acc}\n",
    "        except (RuntimeError, TypeError) as e:\n",
    "            print(f\"\\nERROR during pipeline for {config['name']}: {e}\")\n",
    "            print(\"This run was skipped due to a parameter mismatch in the __main__ configuration.\")\n",
    "            results[config['name']] = {'Link Prediction AUC': 0.0, 'Node Classification Accuracy': 0.0}\n",
    "\n",
    "\n",
    "    print(\"\\n\\n--- FINAL CROSS-TASK EVALUATION SUMMARY ---\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\nModel: {model_name}\")\n",
    "        print(f\"\\tEvaluated on Node Classification, Test Accuracy: {metrics['Node Classification Accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
